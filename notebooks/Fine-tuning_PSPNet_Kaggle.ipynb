{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0636a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "%cd /kaggle/working\n",
    "\n",
    "\n",
    "repo_name = \"benchmarking-generative-models-for-domain-adaptation\"\n",
    "if os.path.exists(repo_name):\n",
    "    shutil.rmtree(repo_name)\n",
    "\n",
    "\n",
    "print(\"Clono la repository...\")\n",
    "!git clone --recursive https://github.com/AntonioRosano/benchmarking-generative-models-for-domain-adaptation.git\n",
    "%cd {repo_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c08ff3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/benchmarking-generative-models-for-domain-adaptation/models/pspnet\n",
    "\n",
    "os.makedirs('initmodel', exist_ok=True)\n",
    "\n",
    "\n",
    "source_path = \"/kaggle/input/datasets/antoniorosano/weights-resnet/resnet50_v2.pth\" \n",
    "destination_path = \"./initmodel/resnet50_v2.pth\" \n",
    "\n",
    "\n",
    "if os.path.exists(source_path):\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    print(f\"Pesi copiati con successo in: {destination_path}\")\n",
    "else:\n",
    "    print(f\"Errore: Non trovo il file in {source_path}\")\n",
    "    print(\"File disponibili in input:\")\n",
    "    for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "        for file in files:\n",
    "            print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89e565",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Applicazione Patch al codice...\")\n",
    "\n",
    "\n",
    "train_py = \"tool/train.py\"\n",
    "if os.path.exists(train_py):\n",
    "    with open(train_py, 'r') as f: txt = f.read()\n",
    "    if \"checkpoint['state_dict']\" in txt and \"strict=False\" not in txt:\n",
    "        txt = txt.replace(\"model.load_state_dict(checkpoint['state_dict'])\", \n",
    "                          \"model.load_state_dict(checkpoint['state_dict'], strict=False)\")\n",
    "        with open(train_py, 'w') as f: f.write(txt)\n",
    "        print(\"Patch 'strict=False' applicata a train.py\")\n",
    "\n",
    "\n",
    "dataset_code = \"\"\"\n",
    "import os\n",
    "import os.path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def make_dataset(split='train', data_root=None, data_list=None):\n",
    "    image_label_list = []\n",
    "    list_read = open(data_list).readlines()\n",
    "    for line in list_read:\n",
    "        line = line.strip().split(' ')\n",
    "        image_label_list.append((line[0], line[1])) \n",
    "    return image_label_list\n",
    "\n",
    "class SemData(Dataset):\n",
    "    def __init__(self, split='train', data_root=None, data_list=None, transform=None):\n",
    "        self.split = split\n",
    "        self.data_list = make_dataset(split, data_root, data_list)\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.data_list)\n",
    "    def __getitem__(self, index):\n",
    "        image_path, label_path = self.data_list[index]\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = np.float32(image)\n",
    "        label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if label.ndim == 3: label = label[:, :, 0]\n",
    "        # Sanitizzazione: tutto ciò che è >= 25 diventa ignore (255)\n",
    "        label[label >= 25] = 255 \n",
    "        if self.transform is not None: image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "\"\"\"\n",
    "with open(\"util/dataset.py\", \"w\") as f: f.write(dataset_code)\n",
    "print(\"Patch Dataset applicata.\")\n",
    "\n",
    "# 3. Patch transform.py (Compatibility Fix)\n",
    "trans_py = \"util/transform.py\"\n",
    "if os.path.exists(trans_py):\n",
    "    with open(trans_py, \"r\") as f: c = f.read()\n",
    "    with open(trans_py, \"w\") as f: f.write(c.replace(\"collections.Iterable\", \"collections.abc.Iterable\"))\n",
    "    print(\"Patch Transform applicata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79fd3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "\n",
    "GENERATED_IMGS_DIR = \"/kaggle/input/datasets/antoniorosano/translate-img/ego_cut_kaggle/test_latest/images/fake_B\" \n",
    "\n",
    "REAL_MASKS_DIR = \"/kaggle/input/datasets/antoniorosano/ego-ch/EGO-CH-OBJ-SEG/real/train/labels\"\n",
    "\n",
    "\n",
    "LIST_DIR = \"lists/ego_ch_finetune\"\n",
    "os.makedirs(LIST_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Cerco immagini in: {GENERATED_IMGS_DIR}\")\n",
    "gen_images = glob.glob(os.path.join(GENERATED_IMGS_DIR, \"*\"))\n",
    "pairs = []\n",
    "\n",
    "print(f\"   Trovati {len(gen_images)} file.\")\n",
    "\n",
    "for img_path in gen_images:\n",
    "    basename = os.path.basename(img_path)\n",
    "    clean_name = basename.replace(\"_fake_B\", \"\").replace(\"_real_A\", \"\").replace(\".png\", \"\").replace(\".jpg\", \"\")\n",
    "    name_no_ext = os.path.splitext(clean_name)[0]\n",
    "    \n",
    "    mask_path = os.path.join(REAL_MASKS_DIR, name_no_ext + \".png\")\n",
    "    \n",
    "    if not os.path.exists(mask_path):\n",
    "         mask_path = os.path.join(REAL_MASKS_DIR, clean_name + \".png\")\n",
    "\n",
    "    if os.path.exists(mask_path):\n",
    "        pairs.append(f\"{img_path} {mask_path}\")\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(pairs)\n",
    "split = int(len(pairs) * 0.9)\n",
    "train_list = pairs[:split]\n",
    "val_list = pairs[split:]\n",
    "\n",
    "with open(f\"{LIST_DIR}/train.txt\", \"w\") as f: f.write(\"\\n\".join(train_list))\n",
    "with open(f\"{LIST_DIR}/val.txt\", \"w\") as f: f.write(\"\\n\".join(val_list))\n",
    "\n",
    "print(f\"Liste create: {len(train_list)} train, {len(val_list)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6ba50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "PRETRAINED_PATH = \"/kaggle/input/datasets/antoniorosano/psp-weights/PSPNet_logs/train_epoch_30.pth\"\n",
    "\n",
    "\n",
    "config_content = f\"\"\"\n",
    "DATA:\n",
    "  data_root: /\n",
    "  train_list: /kaggle/working/benchmarking-generative-models-for-domain-adaptation/models/pspnet/{LIST_DIR}/train.txt\n",
    "  val_list: /kaggle/working/benchmarking-generative-models-for-domain-adaptation/models/pspnet/{LIST_DIR}/val.txt\n",
    "  classes: 25\n",
    "\n",
    "TRAIN:\n",
    "  arch: psp\n",
    "  layers: 50\n",
    "  sync_bn: False\n",
    "  train_h: 473\n",
    "  train_w: 473\n",
    "  scale_min: 0.5\n",
    "  scale_max: 2.0\n",
    "  rotate_min: -10\n",
    "  rotate_max: 10\n",
    "  zoom_factor: 8\n",
    "  ignore_label: 255\n",
    "  aux_weight: 0.4\n",
    "  train_gpu: [0]\n",
    "  workers: 2\n",
    "  batch_size: 4\n",
    "  batch_size_val: 4\n",
    "  \n",
    "  # --- SETTAGGI FINE TUNING ---\n",
    "  base_lr: 0.0005     # Molto basso per raffinare\n",
    "  epochs: 50          # +20 epoche\n",
    "  start_epoch: 30     # Riprendiamo da qui\n",
    "  resume: {PRETRAINED_PATH}\n",
    "  # --------------------------\n",
    "  \n",
    "  power: 0.9\n",
    "  momentum: 0.9\n",
    "  weight_decay: 0.0001\n",
    "  manual_seed: 0\n",
    "  print_freq: 10\n",
    "  save_freq: 5\n",
    "  save_path: exp/ego_ch_finetune/pspnet50/model\n",
    "  weight: \n",
    "  evaluate: True\n",
    "\n",
    "Distributed:\n",
    "  dist_url: tcp://127.0.0.1:6789\n",
    "  dist_backend: 'nccl'\n",
    "  multiprocessing_distributed: False\n",
    "  world_size: 1\n",
    "  rank: 0\n",
    "  use_apex: False\n",
    "  opt_level: 'O0'\n",
    "  keep_batchnorm_fp32: None\n",
    "  loss_scale: None\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs(\"config/ego_ch_finetune\", exist_ok=True)\n",
    "with open(\"config/ego_ch_finetune/pspnet50.yaml\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "    \n",
    "print(\"Configurazione salvata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7501d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/benchmarking-generative-models-for-domain-adaptation/models/pspnet\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.getcwd()\n",
    "\n",
    "!pip install tensorboardX\n",
    "!python tool/train.py --config config/ego_ch_finetune/pspnet50.yaml"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
